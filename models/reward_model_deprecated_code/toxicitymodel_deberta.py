# -*- coding: utf-8 -*-
"""rm_rlaif.ipynb

Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1VSnuF5gHgglJdrvIwmjTclVVBuwVaKpK
    
    
# RLAIF for Harmful RM
Here we will train a reward model on AI and human-annotated pairs of LLM outputs which will rate how harmful an output is. The reward will be higher for a more harmful output.
"""
"""Conclusion,

# Reward Modeling (from AI/human feedback data)

Now we should have data pairs in the form (output_1, output_2, preference [0,1,2]).
"""

#import os
#os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
#os.environ["CUDA_VISIBLE_DEVICES"]="0"

import pandas as pd
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments
from datasets import Dataset
import torch
from peft import LoraConfig, TaskType
from trl import RewardTrainer, RewardConfig

#torch.cuda.set_device(1) # set device number here

train_data = Dataset.from_pandas(pd.read_csv("toxic_aria_train_data.csv"))

model_name = "microsoft/deberta-v3-xsmall"
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = model.config.eos_token_id

def preprocess(examples):
    kwargs = {"padding": "max_length", "truncation": True, "max_length": 400, "return_tensors": "pt"}

    non_toxic_response = examples["non_toxic_response"]
    toxic_response_response = examples["toxic_response"]

    # Then tokenize these modified fields.
    tokens_non_toxic = tokenizer.encode_plus(non_toxic_response, **kwargs)
    tokens_toxic = tokenizer.encode_plus(toxic_response_response, **kwargs)

    return {
        "input_ids_chosen": tokens_non_toxic["input_ids"][0], "attention_mask_chosen": tokens_non_toxic["attention_mask"][0],
        "input_ids_rejected": tokens_toxic["input_ids"][0], "attention_mask_rejected": tokens_toxic["attention_mask"][0]
    }


formatted_train_data = train_data.map(preprocess)
# formatted_test_data = test_data.map(preprocess)

#peft_config = LoraConfig(
#    task_type=TaskType.SEQ_CLS,
#    inference_mode=False,
#    r=8,
#    lora_alpha=32,
#    lora_dropout=0.1,
#    )

training_args = TrainingArguments(
    output_dir=f"./content/deberta-v3-xsmall",
    per_device_train_batch_size=10,
    do_eval=False,
    save_strategy="steps",
    logging_strategy="steps",
    logging_steps=200,
    max_steps = 2000,
    save_steps = 500,
    learning_rate = 5e-5,
)

trainer = RewardTrainer(
    model=model,
    args=training_args,
    tokenizer=tokenizer,
    train_dataset=formatted_train_data,
#    peft_config=peft_config,
)

trainer.train()